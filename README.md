<div align="center">

# ğŸ® DeepRL DQN Benchmark

### Deep Q-Network vs Double DQN:  A Comprehensive PyTorch Comparison

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29+-0081A5?style=for-the-badge&logo=openaigym&logoColor=white)](https://gymnasium.farama.org)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

*A research-grade implementation of DQN and DDQN algorithms for classic control tasks*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Results](#-observed-results) â€¢ [ğŸ“– Documentation](#-overview) â€¢ [ğŸ¤ Contributing](#-contributing)

</div>

---

## ğŸ“– Overview

This repository provides a **research-grade implementation** of DQN and DDQN algorithms for discrete and discretized-continuous action spaces. The project demonstrates the complete RL training pipeline with modular, well-documented code.

### âœ¨ Key Highlights

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Algorithm Comparison** | Side-by-side comparison of DQN vs DDQN performance |
| ğŸ¯ **Multiple Environments** | Support for `CartPole-v1`, `Acrobot-v1`, `MountainCar-v0`, `Pendulum-v1` |
| ğŸ’¾ **Experience Replay** | Configurable buffer sizes (50K-100K transitions) |
| ğŸ¯ **Target Networks** | Periodic updates for training stability |
| ğŸ” **Îµ-Greedy Exploration** | Exponential decay with customizable parameters |
| ğŸ“Š **W&B Integration** | Optional Weights & Biases experiment tracking |
| ğŸ¬ **Video Recording** | Automated recording of evaluation episodes |

---

## ğŸ§  Problem Statement

Reinforcement learning agents must learn optimal policies through trial-and-error interactions with an environment. Standard Q-learning suffers from **overestimation bias** when using function approximation. 

### Challenges Addressed

| Challenge | Description | Solution |
|-----------|-------------|----------|
| âš ï¸ **Value Overestimation** | DQN uses the same network for action selection and evaluation | DDQN decouples these operations |
| ğŸ“‰ **Training Instability** | Correlated sequential experiences destabilize training | Experience replay + target networks |
| ğŸ¯ **Sparse Rewards** | Environments like MountainCar provide minimal feedback | Reward shaping techniques |
| ğŸ”„ **Continuous Actions** | Q-learning requires discrete actions | Action discretization |

> ğŸ’¡ **DDQN Solution**: Decouples action selection (online network) from action evaluation (target network), resulting in more stable and often superior learning.

---

## ğŸ”„ RL System Pipeline

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           TRAINING PIPELINE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     1. Environment Initialization        â”‚
              â”‚    (Gymnasium:  CartPole, Acrobot, etc.)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     2. State Observation (s_t)           â”‚
              â”‚    (Position, velocity, angles, etc.)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     3. Action Selection (Îµ-greedy)       â”‚
              â”‚    Explore:  random | Exploit: argmax Q   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     4. Execute Action in Environment     â”‚
              â”‚    Receive:  reward (r), next state (s')  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     5. Store Transition in Replay Buffer â”‚
              â”‚    (s, a, r, s', done) â†’ Memory          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     6. Sample Mini-Batch from Buffer     â”‚
              â”‚    Random sampling breaks correlation    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     7. Compute TD Target                 â”‚
              â”‚    DQN:   y = r + Î³ * max_a' Q_target(s') â”‚
              â”‚    DDQN: y = r + Î³ * Q_target(s', argmax â”‚
              â”‚                      Q_online(s'))       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     8. Update Online Q-Network           â”‚
              â”‚    Minimize loss: L = (Q(s,a) - y)Â²      â”‚
              â”‚    Backpropagation + Adam optimizer      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     9. Periodic Target Network Sync      â”‚
              â”‚    Î¸_target â† Î¸_online (every N steps)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    10. Decay Exploration Rate (Îµ)        â”‚
              â”‚    Îµ = max(Îµ_min, Îµ * decay_rate)        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Episode Complete â”‚
                         â”‚  Loop to Step 2   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    11. Evaluation & Video Recording      â”‚
              â”‚    Deterministic policy (Îµ = Îµ_min)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<details>
<summary>ğŸ“š <b>Click to expand:  Step-by-Step Explanation</b></summary>

#### Step 1: Environment Initialization
- **Purpose**: Create the simulation environment and extract state/action space specifications
- **Input**: Environment name (e.g., `CartPole-v1`, `MountainCar-v0`)
- **Output**: Environment object, state dimension, action dimension
- **Implementation**: Uses Gymnasium's `gym.make()` API

#### Step 2: State Observation
- **Purpose**:  Capture the current environment state as input to the Q-network

| Environment | State Dimensions | Components |
|-------------|------------------|------------|
| CartPole-v1 | 4 | Position, velocity, pole angle, angular velocity |
| Acrobot-v1 | 6 | cos(Î¸1), sin(Î¸1), cos(Î¸2), sin(Î¸2), Î¸Ì‡1, Î¸Ì‡2 |
| MountainCar-v0 | 2 | Position, velocity (normalized) |
| Pendulum-v1 | 3 | cos(Î¸), sin(Î¸), angular velocity |

#### Step 3: Action Selection (Îµ-Greedy Policy)
```python
if random() < Îµ:
    return random_action()  # Explore
else:
    return argmax(Q_network(state))  # Exploit
```

#### Step 4: Execute Action & Receive Feedback
- Interact with environment to observe consequences of actions
- **Reward Shaping** (MountainCar):
```python
reward = r_env + Î³ * Î± * pos_next - Î± * pos_cur + Î² * |velocity|
if goal_reached:
    reward += 100
```

#### Step 5: Experience Replay Memory
```python
class ReplayBuffer:
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
```

#### Step 6-11: Training Loop
- Mini-batch sampling, TD target computation, network updates, and evaluation

</details>

---

## âš”ï¸ DQN vs DDQN Comparison

<table>
<tr>
<th>Aspect</th>
<th>ğŸ”µ DQN</th>
<th>ğŸŸ¢ DDQN</th>
</tr>
<tr>
<td><b>Target Computation</b></td>
<td><code>max Q_target(s', a')</code></td>
<td><code>Q_target(s', argmax Q_online(s', a'))</code></td>
</tr>
<tr>
<td><b>Overestimation</b></td>
<td>âŒ High (same network selects & evaluates)</td>
<td>âœ… Reduced (decoupled selection/evaluation)</td>
</tr>
<tr>
<td><b>Stability</b></td>
<td>âš ï¸ Moderate</td>
<td>âœ… Improved</td>
</tr>
<tr>
<td><b>Sample Efficiency</b></td>
<td>Good</td>
<td>Often better on complex tasks</td>
</tr>
</table>

### ğŸ§® The Overestimation Problem

DQN's max operator introduces a positive bias: 
```
E[max(Qâ‚, Qâ‚‚, ..., Qâ‚™)] â‰¥ max(E[Qâ‚], E[Qâ‚‚], ..., E[Qâ‚™])
```

**DDQN's Solution:**
1. **Action Selection**: Use online network â†’ `a* = argmax Q_online(s')`
2. **Action Evaluation**: Use target network â†’ `Q_target(s', a*)`

---

## ğŸ§ª Supported Environments

| Environment | Action Space | State Space | Goal | Max Steps |
|-------------|: ------------:|:-----------:|------|:---------:|
| ğŸ¢ CartPole-v1 | Discrete(2) | Box(4) | Balance pole | 500 |
| ğŸ¤¸ Acrobot-v1 | Discrete(3) | Box(6) | Swing up | 500 |
| ğŸ”ï¸ MountainCar-v0 | Discrete(3) | Box(2) | Reach flag | 200 |
| ğŸ”„ Pendulum-v1 | Box(1) â†’ Discretized | Box(3) | Stay upright | 200 |

---

## ğŸ“Š Observed Results

| Environment | Agent | Mean Reward | Std Dev | Status |
|-------------|: -----:|:-----------:|:-------:|: ------:|
| CartPole-v1 | DQN | ~370 | ~50 | âœ… Solved |
| CartPole-v1 | DDQN | ~195 | ~10 | âœ… Solved |
| Acrobot-v1 | DQN | ~-110 | ~30 | âœ… Good |
| Acrobot-v1 | DDQN | ~-90 | ~20 | âœ… Better |
| Pendulum-v1 | DQN | ~-125 | ~100 | âš ï¸ Variable |
| Pendulum-v1 | DDQN | ~-130 | ~100 | âš ï¸ Variable |

> âš ï¸ **Note**: MountainCar requires extended training (1000+ episodes) and reward shaping for success.

---

## ğŸš€ Quick Start

### Prerequisites

- ğŸ Python 3.10+ (3.11 recommended)
- ğŸ® CUDA-capable GPU (optional, for faster training)

### ğŸ’» Windows Installation

```batch
::  1) Clone the repository
git clone https://github.com/kariem-magdy/DeepRL-DQN-Benchmark.git
cd DeepRL-DQN-Benchmark

:: 2) Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate

:: 3) Install dependencies
pip install --upgrade pip
pip install torch numpy matplotlib gymnasium gymnasium[classic_control] wandb jupyter tqdm moviepy pygame

:: 4) Launch Jupyter Notebook
jupyter notebook
```

### ğŸ§ Linux/macOS Installation

```bash
# 1) Clone the repository
git clone https://github.com/kariem-magdy/DeepRL-DQN-Benchmark.git
cd DeepRL-DQN-Benchmark

# 2) Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3) Install dependencies
pip install --upgrade pip
pip install torch numpy matplotlib gymnasium "gymnasium[classic_control]" wandb jupyter tqdm moviepy pygame

# 4) Launch Jupyter Notebook
jupyter notebook
```

### ğŸ“Š Weights & Biases Setup (Optional)

```bash
wandb login
# Enter your API key when prompted
```

---

## ğŸ“ Usage Examples

### Training Both Agents

```python
# Environments to train on
envs = ["CartPole-v1", "Acrobot-v1", "MountainCar-v0", "Pendulum-v1"]

for env_name in envs:
    # Train DQN
    dqn_agent, dqn_rewards, dqn_meta = train_agent(env_name, "DQN", episodes=100)
    
    # Train DDQN
    ddqn_agent, ddqn_rewards, ddqn_meta = train_agent(env_name, "DDQN", episodes=100)
```

### Custom Configuration

```python
config = {
    "gamma": 0.99,
    "epsilon_start": 1.0,
    "epsilon_min": 0.01,
    "epsilon_decay": 0.995,
    "lr": 1e-3,
    "memory_size": 100000,
    "batch_size": 64
}

agent = DDQNAgent(state_size=4, action_size=2, config=config)
```

### Evaluating a Trained Agent

```python
# Load and evaluate
agent.load("models/CartPole-v1_DDQN.pth")
eval_rewards, videos = evaluate_and_record(
    agent,
    env_name="CartPole-v1",
    actions_list=[0, 1],
    agent_type="DDQN",
    episodes=10
)
print(f"Mean evaluation reward: {np.mean(eval_rewards):.2f}")
```

---

## ğŸ“ Project Structure

```
DeepRL-DQN-Benchmark/
â”‚
â”œâ”€â”€ ğŸ““ final_dqn_ddqn_record_last3. ipynb  # Main experiments notebook
â”‚   â”œâ”€â”€ QNetwork class                    # Neural network architecture
â”‚   â”œâ”€â”€ ReplayBuffer class                # Experience replay implementation
â”‚   â”œâ”€â”€ DQNAgent class                    # DQN algorithm
â”‚   â”œâ”€â”€ DDQNAgent class                   # DDQN algorithm
â”‚   â”œâ”€â”€ train_agent()                     # Training loop
â”‚   â””â”€â”€ evaluate_and_record()             # Evaluation with video
â”‚
â”œâ”€â”€ ğŸ““ updatedWithMountainCar. ipynb       # MountainCar-focused experiments
â”‚
â”œâ”€â”€ ğŸ“„ Assignment 2.pdf                   # Lab handout and references
â”‚
â”œâ”€â”€ ğŸ“‚ models/                            # Saved model weights (generated)
â”‚   â”œâ”€â”€ CartPole-v1_DQN.pth
â”‚   â”œâ”€â”€ CartPole-v1_DDQN.pth
â”‚   â””â”€â”€ ... 
â”‚
â”œâ”€â”€ ğŸ“‚ videos/                            # Evaluation recordings (generated)
â”‚   â””â”€â”€ {env_name}/{agent_type}/*. mp4
â”‚
â””â”€â”€ ğŸ“„ README.md                          # This file
```

---

## âš™ï¸ Hyperparameters

| Parameter | Default | MountainCar | Description |
|-----------|: -------:|:-----------:|-------------|
| `gamma` | 0.99 | 0.99 | Discount factor |
| `epsilon_start` | 1.0 | 1.0 | Initial exploration rate |
| `epsilon_min` | 0.01 | 0.01 | Minimum exploration rate |
| `epsilon_decay` | 0.995 | 0.9995 | Decay multiplier per step |
| `learning_rate` | 1e-3 | 5e-4 | Adam optimizer learning rate |
| `batch_size` | 64 | 64 | Training batch size |
| `memory_size` | 50,000 | 100,000 | Replay buffer capacity |
| `target_update` | Per episode | Every 500-1000 steps | Target network sync frequency |

---

## âš ï¸ Limitations & Known Issues

### Current Limitations

| Limitation | Details |
|------------|---------|
| ğŸ”ï¸ **MountainCar** | Requires reward shaping + extended training (1000+ episodes) |
| ğŸ”„ **Continuous Actions** | Pendulum uses discretized actions, limiting precision |
| ğŸ›ï¸ **Hyperparameter Sensitivity** | Different environments require tuned settings |
| ğŸ“Š **No Prioritized Replay** | Uniform sampling may be sample-inefficient |

### Known Issues

- âš ï¸ Video recording may fail if `moviepy` or `pygame` are not properly installed
- â„¹ï¸ W&B integration is optional; code handles its absence gracefully

---

## ğŸ”® Future Improvements

- [ ] ğŸ“Š **Prioritized Experience Replay (PER)**
- [ ] ğŸ§  **Dueling DQN Architecture**
- [ ] ğŸ”Š **Noisy Networks** for exploration
- [ ] ğŸŒˆ **Rainbow DQN** combination
- [ ] ğŸ“‹ Add proper `requirements.txt`
- [ ] ğŸ”„ Implement soft target updates (Polyak averaging)
- [ ] ğŸ“ˆ Add TensorBoard logging
- [ ] âš¡ Multi-environment parallel training
- [ ] ğŸ¯ Add A2C, PPO for comparison

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/improvement`)
3. ğŸ“ Make changes with clear documentation
4. ğŸ§ª Test on at least one environment (CartPole recommended)
5. ğŸš€ Submit a pull request

> ğŸ’¬ For major changes, please open an issue first to discuss the approach.

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“š References

| Resource | Link |
|----------|------|
| ğŸ“˜ PyTorch RL Tutorial | [pytorch.org](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) |
| ğŸ® Gymnasium Documentation | [gymnasium. farama.org](https://gymnasium.farama.org) |
| ğŸ“Š Weights & Biases Guides | [docs.wandb.ai](https://docs.wandb.ai/guides/track/) |
| ğŸ“„ DQN Paper (Nature 2015) | [nature.com](https://www.nature.com/articles/nature14236) |
| ğŸ“„ DDQN Paper (AAAI 2016) | [arxiv.org](https://arxiv.org/abs/1509.06461) |

---

<div align="center">

**Made with â¤ï¸ for the Deep RL community**

â­ Star this repo if you find it helpful!

</div>
